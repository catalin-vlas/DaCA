<!DOCTYPE html>
<html lang="en-GB">
 <head>
 
   <meta charset="utf-8">
   <meta name="viewport" content="width=device-width">
   <title>DaCA - Web Data Commons Analyzer </title>
   <link rel="stylesheet" href="scholarly.css">
   <link rel="stylesheet" href="node_modules/prismjs/themes/prism-coy.css">
   <script src="node_modules/prismjs/prism.js" defer></script>
   
 </head>
 
 <body prefix="schema: http://schema.org/ xsd: http://www.w3.org/2001/XMLSchema# sa: https://ns.science.ai/">
   
   <header>
     <p class="title">DaCA - Web Data Commons Analyzer</p>
	 <p class="subtitle">Techincal Report</p>
   </header>
  
  <article id="daca" typeof="schema:ScholaryArticle" resource="#">
     <h1> DaCA - Technical Report </h1>
	 
	 <section>
        <ol>
          <li property="schema:author" typeof="sa:ContributorRole">
            <a property="schema:author" typeof="schema:Person">
              <span property="schema:givenName">Vlas</span>
              <span property="schema:familyName">Catalin</span>
            </a>
          </li>
          <li property="schema:author" typeof="sa:ContributorRole">
            <a property="schema:author" typeof="schema:Person">
              <span property="schema:givenName">Hreapca</span>
              <span property="schema:familyName">Aurelian</span>
            </a>
          </li>
          <li property="schema:author" typeof="sa:ContributorRole">
            <a property="schema:author" typeof="schema:Person">
              <span property="schema:givenName">Barcan</span>
              <span property="schema:familyName">Virgil</span>
            </a>
          </li>
        </ol>
        <ol>
          <li id="institution">
            <a href="https://www.info.uaic.ro/bin/Main/" typeof="schema:Corporation">
              <span property="schema:name">Alexandru Ioan Cuza Uneversity, Faculty of Computer Science, Iasi </span>
            </a>
          </li>
        </ol>
	 </section>
	 
	 <section typeof="sa:Abstract" id="abstract">
        <h2>Abstract</h2>
        <p>
          DaCA is a web tool able to perform various processing tasks regarding the meta-data available in RDFa and HTML5 microdata 
          formats provided by the Web Data Commons. Using a modular approach, a minimal set of operations are provided: visualize,
          clasify, compare, and match/align. Various statistics, modeled with the RDF Data Cube vocabulary, are also exposed.
        </p>
      </section>
	  
	  
      <section typeof="sa:MaterialsAndMethods" id="introduction">
        <h2>Introduction</h2>
        <p>
          More and more websites have started to embed structured data describing products, people, organizations, places, events into their HTML pages 
		  using markup standards such as RDFa, Microdata and Microformats. The Web Data Commons project extracts this data from several billion web pages. 
		  The project provides the extracted data for download and publishes statistics about the deployment of the different formats.
		  For example the most recent scan was performed in October 2016 and the results were quite impressive. From over 3.1 billions scanned URLs over 1.2 billions
		  contained RDF data which represents about 38% which is a significant increase comparing with the scan from 2012 where only 12% of scanned URLs contained 
		  RDF data. 
        </p>
		
		<p>
		  Using DaCA you are able to load and analyse knowledge graphs collected by Web Data Commons from the entire internet. This can make you or your applications,
		  better understand and relationate web resources. This information can be used to build smart websites and aplications and even to make buisiness decisions.<br><br>
		
		  To use the tool, a user should create an account or register with facebook for example. After this he will be able to upload or pass direct links to
		  files provided by Web Data Commons in N-Quads format. After the file is uploaded the user will be able to see the list of uploaded files 
		  in his personal dasboard, which will contain for each file some statistics like: number of nodes, number of connected graphs,
		  number of edges, etc. Also phisical information about uploaded files will be provided: internal id, file size, etc.
		  From dashboard page a user will be able to navigate to a visualisation page where he will see the list of graphs contained in the uploaded 
		  dataset, as well as a graphical representation of those graphs. Here he will be able also to perform SPARQL queries against loaded dataset.
		  The tool aslo provides support for RDF graph comparison and match/align. The user can enter comparison page for example choose two graphs from his collections
		  and check if those are isomorphic, or find nodes contained in a graph but not in the other or get nodes which are contained in both graphs. Also the tool gives the
		  posibility to export the comparison results in different formats like RDF, JSON-LD or Turtle... 
		</p>
		
      </section>
	  
	  
	  <section typeof="sa:MaterialsAndMethods" id="visualisation">
        <h2>Visualisation</h2>
        <p>
		  This module should provide a way to visualize RDF graphs/datasets uploaded by user on "daca" website. <br>
          The visualization will be performed on client side using a JS library after getting the graph in a JSON format from "daca" backend.	
		</p>
		
		<h3>Existing tools:</h3>
		<p>
		  <ul>
		    <li>
			   <a href="http://en.lodlive.it/"> LodLive </a> <br> 
			   The basic principle underlying LodLive is to prove that resources published according to the W3C SPARQL standard are easy to reach and understand, with hopes that the LodLive approach can stimulate Public Administrations and large-scale data owners to add their resources to the LOD and share them.
			</li>
			
		    <li> 
			   <a href="https://github.com/gephi/gephi/wiki"> Gephi </a> <br> 
               Gephi is an interactive visualization and exploration platform for all kinds of networks and complex systems, dynamic and hierarchical graphs.
               Gephi can not just visualize graphs but also supports analyzing, layouting and further importing and exporting. There is a <a href="https://wiki.gephi.org/index.php/SemanticWebImport"> Semantic Web Import Extension </a> that allows you to directly query (via SPARQL) or import RDF data. You can install it directly within the program at Plugins.			
			</li>
			
		    <li> 
			   <a href="http://rhizomik.net/html/redefer/rdf2svg-form/"> Rhizomik </a> <br> 
	           Rhizomik is a converter, which can transform a RDF file in a SVG image
			</li>
			
		    <li> <a href="https://visgraph3.github.io/index.html"> Vizgraph3 </a> </li>
		    <li> <a href="http://philogb.github.io/jit/demos.html"> Philogb </a> </li>
		    <li> <a href="http://vowl.visualdataweb.org/v2/"> Vowl </a> </li>			
		  </ul>
		</p>
		
		<h3>JS Libraries:</h3>
		<p>
		  <ul>
		    <li>
			   <a href="http://sigmajs.org/"> Sigmajs </a> <br> 
               Sigma is a JavaScript library dedicated to graph drawing. It makes easy to publish networks on Web pages, and allows developers to integrate network exploration in rich Web applications.
			</li>
			
		    <li> 
			   <a href="https://d3js.org/"> D3js </a> <br> 
               D3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3â€™s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.			
			</li>
			
		    <li> 
			   <a href="https://github.com/ktym/d3sparql"> D3SPARQL </a> <br> 
	           JavaScript library for executing SPARQL query and transforming resulted JSON for visualization in D3.js.
			</li>

		    <li> 
			   <a href="http://visjs.org/"> Visjs </a> <br> 
	           A dynamic, browser based visualization library. The library is designed to be easy to use, to handle large amounts of dynamic data, and to enable manipulation of and interaction with the data. The library consists of the components DataSet, Timeline, Network, Graph2d and Graph3d.
			</li>

		    <li> 
			   <a href="https://stackoverflow.com/questions/7034/graph-visualization-library-in-javascript"> Other </a>
			</li>			
		
		  </ul>
		</p>		
      </section>
	  
	  
	  <section typeof="sa:MaterialsAndMethods" id="visualisation">
        <h2>Comparison</h2>
        <p>
          The input of this module will be two RDF graphs and a set operation like A\B or A &cap; B. Also it can check for graph izomorphism.	
		</p>
		
		<h3>Existing tools:</h3>
		<p>
		  <ul>
		    <li> <a href="https://www.w3.org/2001/sw/wiki/How_to_diff_RDF"> Here is a summary on existing RDF comparison tools </a> </li>
			
		    <li> 
			   <a href="https://github.com/RDFLib/rdflib/blob/master/rdflib/compare.py"> Compare.py </a> <br> 
               A collection of utilities which can be used in RDF graph comparison. Here we can find functions which can help us to determine 
               if two graphs are isomorphic or not, nodes that are present in a graph but not in other and also common nodes.			
			</li>
			
		    <li> 
			   <a href="https://jena.apache.org/"> Apache Jena </a> <br> 
	           Apache Jena is a free and open source framework for building Semantic Web and Linked Data applications. Jena offers support for graph izomorphism checking 
              (<a href="https://github.com/apache/jena/blob/master/jena-cmds/src/main/java/jena/rdfcompare.java"> usage example </a>).
			</li>
			
		    <li> 
			 <a href="http://docs.rdf4j.org/"> RDF4j </a> <br>
			 Another interesting framework which enables users to construct and manipulate RDF models in different ways.
			</li>
			
		  </ul>
		</p>	
      </section>
	  
	  <section typeof="sa:MaterialsAndMethods" id="storage">
        <h2>Storage</h2>
        <p>
          This component will manage storing and offering RDF graphs.
		</p>
		<p>
		  For these tasks the application will use Apache Jena's <a href="https://jena.apache.org/documentation/tdb/">TDB</a> library. TDB is a component of Jena for RDF storage and query.
		</p>
		<p>
		  TDB is a persistent graph storage layer for Jena, allowing for CRUD operations and Transactions. It is a pure-Java, employing memory mapped I/O, a custom implementation of <a href="https://en.wikipedia.org/wiki/B+_tree">B+Trees</a> and optimized range filters for XSD value spaces (integers, decimals, dates, dateTime).
		</p>
		<p>
		A dataset backed by TDB is stored in a single directory in the filing system. A dataset consists of:
		<ul>
			<li>
				<b>The node table</b>
			</li>
				The node table stores the representation of RDF terms, providing two mappings from Node to NodeId and from NodeId to Node.
				The Node to NodeId mapping is used during data loading and when converting constant terms in queries from their Jena Node representation to the TDB-specific internal ids.
				The NodeId to Node mapping is used to turn query results expressed as TDB NodeIds into the Jena Node representation and also during query processing when filters are applied if the whole node representation is needed for testing (e.g. regex).
			<li>
				<b>Triple and Quad indexes</b>
			</li>
				Quads are used for named graphs, triples for the default graph. Triples are held as 3-tuples of NodeIds in triple indexes - quads as 4-tuples. Otherwise they are handled in the same manner.
				The triple table is 3 indexes - there is no distinguished triple table with secondary indexes. Instead, each index has all the information about a triple.
			<li>
				<b>The prefixes table</b>
			</li>
				The prefixes table uses a node table and a index for GPU (Graph->Prefix->URI). It is usually small. It does not take part in query processing. It provides support for Jena's PrefixMappings used mainly for presentation and for serialisation of triples in RDF/XML or Turtle.
		</ul>
		</p>
	  </section>

	  <section typeof="sa:MaterialsAndMethods" id="statistics">
        <h2>Dataset statistics</h2>
        <p>
          This component will manage the collection and offering of dataset statistics.
		</p>
		<p>
			Every time a dataset is uploaded on our website (via UI or API), multi-dimensional data
			regarding this dataset along with its graphs is published as a <a href="https://www.w3.org/TR/vocab-data-cube/">RDF Data Cube</a> to facilitate
			reporting, supporting decisions (inference) and getting insight about data (descriptive statistics).
		</p>
		<p>
			The advantages of using RDF Data Cube are that it can be queried via <a href="https://www.w3.org/TR/rdf-sparql-query/">SparQL</a> and also can be linked to other concepts/datasets.
		</p>
		<p>
		The components of a RDF Data Cube:
		<ul>
			<li>
				<b>Dimensions</b>
			</li>
				Used to identify observations.
			<li>
				<b>Attributes</b>
			</li>
				Used to qualify and interpret the observed values.
			<li>
				<b>Measures</b>
			</li>
				Used to represent the observation.
		</ul>
		</p>
		<p>
			For example, if the aim is to retrieve statistics related to number of triples we can consider as
	dimensions: graph, node and role (if the node is the subject or the object) of the triple, while the measure
	is every entry in the data cube, so the number of the triples.
		</p>
		<p>
			The visualization and the export of these statistics is available using the corresponding modules presented above.
		</p>
	  </section>

	  <section typeof="sa:Conclusion" id="conclusion">
        <h2>Conclusion</h2>
        <p>
          As web of knowledge is growing fast and RDF graphs become more complex and hard to manipulate, such tools as DaCA will become a <it>must</it> in near future.
		  More complex and efficient algorithms for processing RDF graphs will be required and for sure this will require distributed systems support and cloud availability. 
		  There are some available web tools and descktop applications which can be used for different processing tasks but there is not a generic tool, easy to use, which offers support for all kind of operations in one place. DaCA tries to acomplish this task and provides a platform where each user can keep track of his RDF files also 
          performing different operations on his graphs using the website or through our API, also having the posibility to visualize loaded graphs and perform SPARQL queries against them.  		  
		</p>  
      </section>
	  
	  <section typeof="sa:ReferenceList" id="references">
        <h2>References</h2>
        <dl>
          <dt>Web Data Commons</dt>
          <dd id="ref-nyt" typeof="schema:ScholarlyArticle" resource="http://webdatacommons.org/structureddata/2016-10/stats/how_to_get_the_data.html">
            <cite property="schema:name"><a href="http://webdatacommons.org/structureddata/2016-10/stats/how_to_get_the_data.html">How to get data</a></cite>.
          </dd>
        </dl>
		
		<dl>
          <dt>Data Cube</dt>
          <dd id="ref-nyt" typeof="schema:ScholarlyArticle" resource="https://www.w3.org/TR/vocab-data-cube/">
            <cite property="schema:name"><a href="https://www.w3.org/TR/vocab-data-cube/">Modeling RDF Stats with Data Cube Vocabulary</a></cite>.
          </dd>
        </dl>
      </section>
	 
  </article>   
 
 </body>

</html>
